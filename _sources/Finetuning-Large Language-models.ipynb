{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb0ef56-487e-4f7a-94eb-1e7c63c93d89",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeff8949-c2e1-49c0-8194-eec7883bce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-community transformers bitsandbytes accelerate langchain-openai langchain evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a185c3fc-7842-419a-b3ce-943315962e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv, os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4069c-f56b-4d88-a326-24a33931107a",
   "metadata": {},
   "source": [
    "# Finetuning large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e6529-460b-4c40-8ea7-ac03db7c99bb",
   "metadata": {},
   "source": [
    "In the previous chapter, we explored the inner workings of large language models and how they can be leveraged for various tasks, such as text generation and sequence classification, through effective prompting and zero-shot capabilities. We also delved into the vast array of pre-trained models available, courtesy of the vibrant community.\n",
    "\n",
    "However, will these pre-trained models exhibit remarkable versatility, their general purpose training may not always be optimized for specific tasks or domains. Fine-tuning emerges as a crucial techique to adapt and refine a language model's understanding to the nuances of a particular dataset or a task\n",
    "\n",
    "Consider the field of medical research, where a language model pre-trained solely on general web text may struggle to perform effectively out-of-the-box. By fine-tuning the model on a corpus of medical literature, its ability to generate relevant medical text or assist in information extraction from healthcare documents can be significantly enhanced.\n",
    "\n",
    "Conversational models present another compelling use case. As discussed earlier, large pre-trained models are primarily trained to predict the next token, which may not seamlessly translate to engaging, conversational interactions. By fine-tuning these models on datasets containing everyday conversations and informal language structures, we can adapt their outputs to emulate the natural flow and nuances of interfaces like ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf6c46-fccf-467b-929f-099e3f2ab754",
   "metadata": {},
   "source": [
    "The primary objective of this chapter is to establish a solid foundation in fine-tuning large language models (LLMs). Consequently, we will delve into the following key areas:\n",
    "\n",
    "- Classifying the topic of a text using a fine-tuned encoder model\n",
    "- Generating text in a particular style using a fine-tuned decoder model\n",
    "- Solving multiple tasks with a single model via instruction fine-tuning\n",
    "- Parameter-efficient fine-tuning techniques that enable training on smaller GPUs\n",
    "- Techniques for reducing computational requirements during model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c46e1f-8488-414a-93ea-a1fbd4977c4e",
   "metadata": {},
   "source": [
    "Through this comprehensive exploration, you will gain insights into tailoring language models to excel in specific tasks and domains, unleashing their true potential for a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2096b4b-89b4-44cb-9cab-26faedbb2d72",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5ff98-bdf2-4351-a8c0-5b14c35f33d8",
   "metadata": {},
   "source": [
    "As we discussed in earlier chapters, LLMs are generally used for generative tasks where task is to predict the next token. Other NLP tasks such as text classification, named entity recognition might not be represented easily with the default objective. Here we will see an example of using LLMs for text classification and then further finetuning to improve the metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6324b1-b926-4e13-9c1d-1e396adcd266",
   "metadata": {},
   "source": [
    "### Identify a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4af22a-579d-4181-acc7-75217c4d8ff5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's pick publicly available dataset to demonstrate the technique. Here we'll use AG news dataset, a well known non-commercial dataset used for benchmarking text classification models and researching data mining, information retrieval and data streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ad99c-bfc3-47b8-8819-04af37a400f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's explore the dataset to know about the text and labels. The dataset provides 120,000 training examples, more than enough data to fine-tune a model. Fine-tuning requires very little data compared to pre-training a model and just using few thousand examples should enough to get a good baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6777d4-088d-4930-8bce-38152d9da44a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "raw_datasets = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57762160-3615-4f6b-8a6f-a890b3d52c21",
   "metadata": {},
   "source": [
    "Let's print the first sample from the training dataset. Output shows each sample is a dictionary with two keys: `text` and `label` . The `text` key contains the actual text of the news article, while the `label` key contains an integer representing the category of the article. In this particular example, article is labeled with integer `2`, which corresponds to `business` category according to the dataset's label encoding scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af0ee72-3f4a-48bd-a9b9-88e2c033b0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0fbb9-a60b-44cb-ab5a-195f9344ec6a",
   "metadata": {},
   "source": [
    "Before the era of LLM, we used RNNs or BERT style models to capture the meaning of a sentence and then finetuning for a downstream task. Let's look at some ways to achieve the same results using LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2815ba1-76f0-44d8-8ce6-19f0ebe0686e",
   "metadata": {},
   "source": [
    "But supervised learning is only one option for text classification with LLMs. Unsupervised learning through prompt engineering has emerged as a viable alternative. How well do LLMs perform text classification when guided only by a natural language prompt? Can this approach compete with the results from supervised learning? We explore these questions and more in the next post in our series. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bbe6c-d3b1-4c2f-ba25-368f308bc155",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e9d39-7bc0-456b-abc0-a5d8aeec5c36",
   "metadata": {},
   "source": [
    "Let's look at zero-shot capability of large language models (LLMs) where they can perform a task without explicit training data for that specific task.\n",
    "\n",
    "We first extract the label names from the dataset using `raw_datasets[train].features[label].names` . This gives us the list of label names corresponding to the integer labels in the dataset\n",
    "\n",
    "Next, we create a dictionary `id_to_label` that maps the lowercase label names to their corresponding integer labels.\n",
    "\n",
    "Finally, we modify the dictionary `id_to_label` to expand the `sci/tech` label. This is useful since LLM might be trained with full words rather than abbrevations. We should try to be close to the initial vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76ecb268-5e06-4916-8ce3-ec1af6b63283",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = raw_datasets['train'].features['label'].names\n",
    "id_to_label = {l.lower():i for i,l in enumerate(labels)}\n",
    "id_to_label['science/technology']=3 #expanding one of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b515028c-458b-438a-8821-a6165ea48feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'world': 0,\n",
       " 'sports': 1,\n",
       " 'business': 2,\n",
       " 'sci/tech': 3,\n",
       " 'science/technology': 3}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b060b35-f9a2-400f-90b3-79db62183750",
   "metadata": {},
   "source": [
    "Here, We setup the prompt engineering pipeline for text classification using an LLM. We start by importing the necessary libraries from LangChain, a framework for building applications with LLMs.\n",
    "\n",
    "We define a `ChatPromptTemplate` and `tagging_prompt` tht will be used to construct the prompt for the LLM. The prompt instructs the LLM to extract the news label from the given article based on the `Classification` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "100f3085-dcfc-444f-b000-18e4a5bd5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from enum import Enum\n",
    "from langchain.output_parsers.enum import EnumOutputParser\n",
    "\n",
    "\n",
    "tagging_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Extract the News Label from the following article. Follow the instructions below.\n",
    "\n",
    "Only extract the properties mentioned in the 'Classification' function.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63880a0-599b-48b3-97c2-185ab21d445f",
   "metadata": {},
   "source": [
    "Next, we define a `classification` class that inherits from `BaseModel`. This class represents the structured output format that the LLM will generate. It has a single field `label` of type `str`, which is an enumeration of the four label categories: \n",
    "\n",
    "- World\n",
    "- sports\n",
    "- business\n",
    "- science/technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "319c5005-ebbf-4946-bbf0-3b0e1ee3293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classification(BaseModel):\n",
    "    label:str = Field(enum=['world','sports','business','science/technology']) # Note: Using the expanded label of sci/tech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034645b5-ee6f-4f41-937f-a669e88de5af",
   "metadata": {},
   "source": [
    "We then initialize the LLM using ChatOpenAI from the langchain_openai library. In this example, we use the gpt-4-turbo model from OpenAI. We set the temperature parameter to 0 to obtain deterministic outputs from the LLM. Additionally, we use the with_structured_output method to specify that the LLM should generate outputs in the format defined by the Classification class1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0e17f9-6cad-4bcc-a023-42258c3432f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4-turbo\").with_structured_output(\n",
    "    Classification\n",
    ")\n",
    "\n",
    "\n",
    "tagging_chain = tagging_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98985671-ba1f-4fbf-8586-e6b4d309b391",
   "metadata": {},
   "source": [
    "## Invoking the LLM for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90bcfb-5e23-431a-8802-b0b87e1361f3",
   "metadata": {},
   "source": [
    "Here, we invoke the LLM for text classification using `tagging_chain` we created earlier. First, we define the input text `inp` which we want to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84682815-b58c-4c7e-9446-438cee12d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(label='science/technology')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = \"Iphone is a great technology. Everybody should use it.\"\n",
    "tagging_chain.invoke({\"input\": inp})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472bb559-476a-48e7-9e13-a4eeffedbabf",
   "metadata": {},
   "source": [
    "In the example above, LLM correctly classifies the input text as `science/technology` demonstrating the effectiveness of prompt engineering for text classification using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40bfb2-66f3-4e86-a7d1-276c6a93d576",
   "metadata": {},
   "source": [
    "We can then repeat this process to all the test samples. Below, we call `tagging_chain.invoke` for all the examples and save output as one of the feature in dataset. Feature is stored as `result` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559973d-4989-4ed2-bdf7-8219e070bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function process_text at 0x7fc83274ad40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc2cd4779454d8fbf67ad24520b4fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_text(example):\n",
    "    try:\n",
    "        out=tagging_chain.invoke({'input':inp})\n",
    "        example['result']=out.label\n",
    "    except:\n",
    "        example['result']= ''\n",
    "    return example\n",
    "dataset_processed = raw_datasets['test'].map(process_text,num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd30467-5c6d-46a7-b4ad-fbf67a895bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed.save_to_disk('data/processed.hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fafc5d-6f17-4ab0-b939-ce601e6bd796",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52daa703-4f9c-4936-ae93-839c005f66fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\\\privately funded suborbital space flight, has officially announced the first\\\\launch date for its manned rocket.',\n",
       " 'label': 3,\n",
       " 'result': 'science/technology'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2961995b-47c8-47bb-a0cd-bd14d80941d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1743339559.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    predictions = [id_to_label[p] for p in dataset_processed['result'] if p !='' else 1]\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "references = [r for r in dataset_processed['label']]\n",
    "predictions = [id_to_label[p] for p in dataset_processed['result'] if p !='' else 1]\n",
    "accuracy = accuracy_metric.compute(references=references,predictions=predictions)\n",
    "print('accuracy {}'.format(accuracy['accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297c7de-a4cb-4bbe-8a5d-c4a362e700c2",
   "metadata": {},
   "source": [
    "## Finetuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb1e74-a704-46ef-949e-268549f51e26",
   "metadata": {},
   "source": [
    "There are two main approaches to fine-tuning large language models (LLMs) for text classification tasks.\n",
    "\n",
    "1. **Building an Entire Domain-specific Model from scratch**\n",
    "\n",
    "- This approach involves training a foundational model entirely on industry-specific knowledge and data, using self-supervised learning techniques like next-token prediction and masking\n",
    "\n",
    "- It requires a massive amount of domain specific data and significant computational resources\n",
    "\n",
    "- An example of this approach is **BloombergGPT** which was trained on decades of financial data, requiring $2.7 million and 53 days of training\n",
    "\n",
    "- The advantage of this approach is that the resulting model is highly specialized and tailored to the specific domain, potentially leading to better performance on domain specific tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0a37c-22b8-4beb-8319-8cc2251f3ba5",
   "metadata": {},
   "source": [
    "2. **Finetuning a pretrained LLM**\n",
    "\n",
    "- This approach involves taking a pre-trained LLM such as GPT OR BERT, and fine tuning it on a smaller, domain specific dataset.\n",
    "  \n",
    "- It requires less data computation, and time compared to training from scratch, making it more efficient and cost-effective option\n",
    "\n",
    "  \n",
    "- Various techniques can be employed to enhance the fine-tuning process, such as transfer learning, retrieval-augmented generation (RAG), and a multi-task learning\n",
    "\n",
    "  \n",
    "- RAG combines the strengths of pre-trained models and information retrieval systems, enabling the model to retrieve and incorporate domain-specific knowledge during inference\n",
    "\n",
    "  \n",
    "- Multi-task learning involves training a single model on multiple related tasks simultaneously, allowing the model to learn shared representations and benefit from task synergies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22862624-c493-497e-957f-8fa5d409783b",
   "metadata": {},
   "source": [
    "Let's see how we can fine-tune LLAMA 3 model for text classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262148c9-822b-4e78-af2e-1d94f3725796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9a4495-61d9-47ce-9993-f35eec4acc1b",
   "metadata": {},
   "source": [
    "### Rationale for different approaches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04854885-cb42-470e-9349-651aa5bafc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f694dd01-52fe-4eb1-b0f1-24c2e178547f",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae90a3-15d8-4a14-be75-de2715959462",
   "metadata": {},
   "source": [
    "- Text classification using large language models\n",
    "  \n",
    "https://aclanthology.org/2023.findings-emnlp.603.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a747a-3608-4ad1-acc8-20e4ea559a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
