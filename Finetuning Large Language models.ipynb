{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f4069c-f56b-4d88-a326-24a33931107a",
   "metadata": {},
   "source": [
    "# Finetuning large language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268e6529-460b-4c40-8ea7-ac03db7c99bb",
   "metadata": {},
   "source": [
    "In the previous chapter, we explored the inner workings of large language models and how they can be leveraged for various tasks, such as text generation and sequence classification, through effective prompting and zero-shot capabilities. We also delved into the vast array of pre-trained models available, courtesy of the vibrant community.\n",
    "\n",
    "However, will these pre-trained models exhibit remarkable versatility, their general purpose training may not always be optimized for specific tasks or domains. Fine-tuning emerges as a crucial techique to adapt and refine a language model's understanding to the nuances of a particular dataset or a task\n",
    "\n",
    "Consider the field of medical research, where a language model pre-trained solely on general web text may struggle to perform effectively out-of-the-box. By fine-tuning the model on a corpus of medical literature, its ability to generate relevant medical text or assist in information extraction from healthcare documents can be significantly enhanced.\n",
    "\n",
    "Conversational models present another compelling use case. As discussed earlier, large pre-trained models are primarily trained to predict the next token, which may not seamlessly translate to engaging, conversational interactions. By fine-tuning these models on datasets containing everyday conversations and informal language structures, we can adapt their outputs to emulate the natural flow and nuances of interfaces like ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf6c46-fccf-467b-929f-099e3f2ab754",
   "metadata": {},
   "source": [
    "The primary objective of this chapter is to establish a solid foundation in fine-tuning large language models (LLMs). Consequently, we will delve into the following key areas:\n",
    "\n",
    "- Classifying the topic of a text using a fine-tuned encoder model\n",
    "- Generating text in a particular style using a fine-tuned decoder model\n",
    "- Solving multiple tasks with a single model via instruction fine-tuning\n",
    "- Parameter-efficient fine-tuning techniques that enable training on smaller GPUs\n",
    "- Techniques for reducing computational requirements during model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c46e1f-8488-414a-93ea-a1fbd4977c4e",
   "metadata": {},
   "source": [
    "Through this comprehensive exploration, you will gain insights into tailoring language models to excel in specific tasks and domains, unleashing their true potential for a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2096b4b-89b4-44cb-9cab-26faedbb2d72",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5ff98-bdf2-4351-a8c0-5b14c35f33d8",
   "metadata": {},
   "source": [
    "As we discussed in earlier chapters, LLMs are generally used for generative tasks where task is to predict the next token. Other NLP tasks such as text classification, named entity recognition might not be represented easily with the default objective. Here we will see an example of using LLMs for text classification and then further finetuning to improve the metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6324b1-b926-4e13-9c1d-1e396adcd266",
   "metadata": {},
   "source": [
    "### Identify a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4af22a-579d-4181-acc7-75217c4d8ff5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's pick publicly available dataset to demonstrate the technique. Here we'll use AG news dataset, a well known non-commercial dataset used for benchmarking text classification models and researching data mining, information retrieval and data streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ad99c-bfc3-47b8-8819-04af37a400f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's explore the dataset to know about the text and labels. The dataset provides 120,000 training examples, more than enough data to fine-tune a model. Fine-tuning requires very little data compared to pre-training a model and just using few thousand examples should enough to get a good baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6777d4-088d-4930-8bce-38152d9da44a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6007f8422bd43db8098c47cc581d4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96db46bc6b24f8e95276f10e412f3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93be8e75b1064cdfad1bd31713cc5a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce19b556ead46ea908836dbcf6b2258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1706054af6cc4732a6713d181b541d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"ag_news\")\n",
    "raw_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af20639-61aa-4eca-a720-393b7d9d5cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0fbb9-a60b-44cb-ab5a-195f9344ec6a",
   "metadata": {},
   "source": [
    "Before the era of LLM, we used RNNs or BERT style models to capture the meaning of a sentence and then finetuning for a downstream task. Let's look at some ways to achieve the same results using LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2815ba1-76f0-44d8-8ce6-19f0ebe0686e",
   "metadata": {},
   "source": [
    "But supervised learning is only one option for text classification with LLMs. Unsupervised learning through prompt engineering has emerged as a viable alternative. How well do LLMs perform text classification when guided only by a natural language prompt? Can this approach compete with the results from supervised learning? We explore these questions and more in the next post in our series. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bbe6c-d3b1-4c2f-ba25-368f308bc155",
   "metadata": {},
   "source": [
    "## Zeroshot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f712b6e-fdf1-459b-8f14-e1dc72651681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cddbcfd-1f48-4802-bb5e-067e698ccfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4f54e-2a4c-433d-88e1-702e73c19567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f694dd01-52fe-4eb1-b0f1-24c2e178547f",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae90a3-15d8-4a14-be75-de2715959462",
   "metadata": {},
   "source": [
    "- Text classification using large language models\n",
    "  \n",
    "https://aclanthology.org/2023.findings-emnlp.603.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c02bc2-a568-4169-8b31-07d6d2f8b02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
