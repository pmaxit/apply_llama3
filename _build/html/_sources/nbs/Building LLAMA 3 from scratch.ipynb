{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0193a7e2-d10a-4560-b326-cc1f78311088",
   "metadata": {},
   "source": [
    "# Building LLAMA 3 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ec0b42-1701-4d06-a59e-1e543835d29d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "from tokenizers import Tokenizer, normalizers, pre_tokenizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Digits, Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling, DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast, CausalLMOutputWithCrossAttentions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc06f6-f83c-4df4-b86b-49d2129b1dd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This chapter guides you through the process of building a LLAMA 3 model from scratch using PyTorch and Hugging Face Transformers. While we won't replicate the full scale of LLAMA 3 due to computational constraints, you'll gain a solid understanding of the core concepts and implementation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359314f-8637-4922-aa52-8219c3183142",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Chapter Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a519b77-b590-4cd4-80c4-897b8d226561",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. Building block for a language model\n",
    "2. N-gram language model\n",
    "3. RNN neural network using attention\n",
    "4. Transformer Architecture: The Foundation\n",
    "5. Tokenization and Data Preparation\n",
    "6. The Decoder-Only LLAMA Model\n",
    "7. Training with PyTorch\n",
    "8. (Optional) Using Hugging Face Transformers\n",
    "9. (Optional) converting it into a Chat like format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73079c5e-3088-478b-848f-eacc9c6fa15b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Imagine a young apprentice learning at the feet of Shakespeare, soaking in his vast knowledge word by word. Over time, the apprentice learns to anticipate the next word, the next phrase, the next line of verse. This is the essence of a language model - a system that predicts the next word, given the ones that come before.\n",
    "\n",
    "In today's digital world, language models are powered by algorithms and trained on massive datasets of text, much like that apprentice studying Shakespeare's plays. They've become essential tools for a variety of tasks:\n",
    "\n",
    "- Writing Assistance: They help us write emails, craft essays, and even generate creative content.\n",
    "- Translation: They bridge language barriers by translating text from one language to another.\n",
    "- Conversation: They power chatbots and voice assistants, engaging in conversations with us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0546d2-d34c-4374-b7ed-bb8cf57bd711",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Building block\n",
    "\n",
    "At its core, a langauge model is a statistical model. It analyzes the patterns and probabilities of words occuring together in a vast corpus of text. The more data it's exposed to, the better it becomes at predicting the next word in a sequence.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "- **Tokenization**: The model breaks down text into smaller units called tokens (words, punctuation, etc.).\n",
    "- **Pattern Recognition**: It learns the relationships between these tokens, understanding which words are likely to follow others.\n",
    "- **Prediction**: Given a sequence of words, it calculates the probability of different words coming next and chooses the most likely one.\n",
    "\n",
    "### Different Flavors of Language Models:\n",
    "\n",
    "1. **N-gram Models**: These simpler models look at a fixed number of previous words (bigrams consider two, trigrams consider three) to predict the next.\n",
    "2. **Neural Network Models**: These more sophisticated models use artificial neural networks to capture complex patterns in language.\n",
    "3. **Transformer Models**: The latest breakthrough, these models use attention mechanisms to weigh the importance of different words in a sequence, leading to remarkable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c368ed5-65cd-41dd-97fb-fe1d399fe10d",
   "metadata": {},
   "source": [
    "## N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180340f-71be-4aff-961a-3b22375bd169",
   "metadata": {},
   "source": [
    "An n-gram is a sequence of n words. For instance, \"please turn\" and \"turn your\" are bigrams (2-grams), while \"please turn your\" is a trigram (3-gram). N-gram models estimate the probability of a word given the preceding n-1 words.\n",
    "\n",
    "To calculate the probability of a word w given a history h, we can use relative frequency counts from a large corpus:\n",
    "\n",
    "$P(w|h) = C(hw) / C(h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede44c2-421e-4e72-8e30-198a1a4ec405",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- P(w|h) is the probability of word w given history h.\n",
    "- C(hw) is the count of the sequence hw in the corpus.\n",
    "- C(h) is the count of the history h in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425c950-f135-4764-9091-d0c76924cb87",
   "metadata": {},
   "source": [
    "However, this approach is limited due to the vastness and creativity of language. Many possible word sequences might not exist in even the largest corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5241c8-8dd1-42e0-a342-35c521972611",
   "metadata": {},
   "source": [
    "### Bi-gram model using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5281958-543a-418b-9f77-e30e9619279c",
   "metadata": {},
   "source": [
    "Here, we will implement bi-gram model using pytorch. Although simple but bigram model can surprise the readers with its surprising predictive power and ability to capture meaningful patterns in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa02342-4170-42b1-99f9-61cdd23095c0",
   "metadata": {},
   "source": [
    "### Bigram model\n",
    "\n",
    "A bigram model operates on fundamental premise: the probability of a word appearing in a text sequence is heavily influences by the word that preceeds it. By analyzing the large corpora of text, we can calculate the additional probabilities of a word pairs. For instance, the probability of encountering the word 'morning' given the preceeding word 'good' is relatively high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9ad38-5b83-4b75-9468-82c43fe4b8d1",
   "metadata": {},
   "source": [
    "Let's illustrate this concept with an example using the following text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be69f4-d525-4569-9f62-74bcd66cce84",
   "metadata": {},
   "source": [
    "\"The cat sat on the mat. The dog chased the cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214b1346-c5b1-4a9e-bcfa-076636d72d25",
   "metadata": {},
   "source": [
    "**1. Tokenization**\n",
    "- Split the corpus into individual words: [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \"the\", \"dog\", \"chased\", \"the\", \"cat\"]\n",
    "\n",
    "**2. Create bi-gram pairs**\n",
    "- Pair consecutive words: [(\"the\", \"cat\"), (\"cat\", \"sat\"), (\"sat\", \"on\"), (\"on\", \"the\"), (\"the\", \"mat\"), (\"mat\", \"the\"), (\"the\", \"dog\"), (\"dog\", \"chased\"), (\"chased\", \"the\"), (\"the\", \"cat\")]\n",
    "\n",
    "**3. Calculate probabilities**\n",
    "- Count the occurence of each bi-gram pair\n",
    "- Calculate the probability of second word given the first word\n",
    "  e.g. $$P( cat | the ) = 2/4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a970dc8-c688-4851-86fe-c8505bb8e578",
   "metadata": {},
   "source": [
    "### Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6730e759-6db0-44a9-b81d-e3e008827676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "corpus = \"the cat sat on the mat the dog chased the cat\"\n",
    "words = corpus.split()\n",
    "vocab = list(set(words))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Build bi-gram matrix (replace with actual count calculations)\n",
    "bigram_counts = torch.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "for i in range(len(words)-1):\n",
    "    bigram_counts[word_to_idx[words[i]], word_to_idx[words[i+1]]] += 1\n",
    "\n",
    "# Normalize to get probabilities\n",
    "bigram_probs = bigram_counts / bigram_counts.sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fd5b4-3c11-45ed-a1d3-1d636a2d516a",
   "metadata": {},
   "source": [
    "With our bi-gram model in hand, we can now generate text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23747b0-86a6-4041-845f-df5e157ee8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat sat on the dog\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(start_word, length):\n",
    "    generated_text = [start_word]\n",
    "    current_word = start_word\n",
    "\n",
    "    for _ in range(length-1):\n",
    "        next_word_idx = torch.multinomial(bigram_probs[word_to_idx[current_word]], 1).item()\n",
    "        next_word = vocab[next_word_idx]\n",
    "        generated_text.append(next_word)\n",
    "        current_word = next_word\n",
    "\n",
    "    return \" \".join(generated_text)\n",
    "\n",
    "print(generate_text(\"cat\", 5)) # Example output: \"cat sat on the dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cc1e69-0b55-4f94-941c-544079ca7815",
   "metadata": {},
   "source": [
    "### Limitations & Enhancements\n",
    "While our bigram model demonstrates the concept, it has limitations due to its simplicity. Real-word text generation often requires more sophisticated models like Recurrent Neural Networks (RNNs) or Transformers. However the bi-gram model serves as a foundational stepping stone for understanding the underlying principles of text generation\n",
    "\n",
    "In the next section, we will delve into more advanced techniques and explore how to build upon this basic model to create more sophisticated text generation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09a93c-181d-4325-978d-c4b8d9d5e9b9",
   "metadata": {},
   "source": [
    "## Recurrent Neural network\n",
    "\n",
    "Imagine reading a book. You don't start from scratch with each word; you carry the context of previous sentences in your mind. RNNs emulate this behavior by maintaining a hidden state that evolves as it processes each word in a sequence. This hidden state acts as a memory, encoding information from previous time steps, allowing the model to make predictions based on both the current input and accumulated context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f34d2b-dc56-4f02-8fa5-249c7c341c00",
   "metadata": {},
   "source": [
    "### A simple RNN structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8b79d-3b31-4214-8628-136c7018ad8f",
   "metadata": {},
   "source": [
    "At its core, an RNN consists of a repeating unit (cell) that takes two inputs: the current current word and the previous hidden state. It produces two outputs: an updated hidden state and a prediction for the next word. This structure allows the RNN to process sequences of aribtrary length, making it suitable for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7790f6ca-171a-4f4d-9557-3e35ddba4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.linear(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f1255-0d8b-473c-953c-69738b9db34c",
   "metadata": {},
   "source": [
    "### Training and text generation\n",
    "\n",
    "Training an RNN involves feeding it sequences of text and adjusting its parameters to minimize the difference between its predictions and the actual next words. Once trained, we can generate text by providing a starting word and iteratively sampling from the model's output distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f64fb8-8647-48a2-9df9-d2b2139df715",
   "metadata": {},
   "source": [
    "### Attention Mechanism: Focus where it matters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df51a10-bfc7-4ac1-af28-254d0a075d1a",
   "metadata": {},
   "source": [
    "A crucial enhancement to RNNs is the attention mechanism. In text generation, not all parts of the input sequence are equally important for predicting the next word. Attention allows the model to focus on relevant parts of the input while making predictions. It's like shining a spotlight on specific words or phrases that are most informative for the current context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78b559-c75a-41bd-b703-2463f9abb4ea",
   "metadata": {},
   "source": [
    "Huggingface models need a config object to instantiate the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3daa8474-516c-4d46-8f1c-6ec251e7a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConfig(PretrainedConfig):\n",
    "    model_type = \"custom_attention\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=50257,\n",
    "        hidden_size=124,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761177a-b171-4996-9963-34d241822b0f",
   "metadata": {},
   "source": [
    "Here we define our RNN model with attention. Attentions works by allowing a model to focus on different parts of its input based on the relevance of each part to the task at hand. In essence, it dynamically weights the input elements to emphasize the most important ones for the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1cd31-e4fc-45e6-bb7d-172ef02ee241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, value_dim):\n",
    "        super().__init__()\n",
    "        self.scale = 1./math.sqrt(query_dim)\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        #query = query.unsqueeze(1)\n",
    "        keys = keys.transpose(1,2)\n",
    "        attention = torch.bmm(query, keys)\n",
    "        attention = F.softmax(attention.mul_(self.scale), dim=2)\n",
    "        weighted_values = torch.bmm(attention, values).squeeze(1)\n",
    "        return attention, weighted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8ad9a-72c3-4d35-9c1b-8ffc711c5377",
   "metadata": {},
   "source": [
    "**Forward Pass**\n",
    "\n",
    "This defines the core functionality of the attention module, how it processes input during model's forward pass. It takes three input tensors:\n",
    "\n",
    "- `query` : Query vector ( what model is looking for )\n",
    "- `keys` : A set of key vectors ( what model can attend to )\n",
    "- `values` : The values associated with each key. ( what model will retrieve )\n",
    "\n",
    "The attention mechanism works like a spotlight that helps a computer model focus on the most important parts of its input.\n",
    "\n",
    "- First, it measures how closely a \"query\" (what the model is looking for) matches different \"keys\" (the parts of the input it can focus on). This gives us a bunch of scores.\n",
    "\n",
    "- Next, these scores are turned into probabilities, making sure they add up to one.  These probabilities show how much the model should focus on each key.\n",
    "\n",
    "- Then, the model combines information from each key, but gives more weight to the keys with higher probabilities. This is like putting a stronger spotlight on the more relevant parts.\n",
    "\n",
    "Finally, the model outputs both these weighted values (the information it focused on) and the probabilities themselves, so we can see what the model considered important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8ca4e-8b57-4d23-b0a0-937811173ecb",
   "metadata": {},
   "source": [
    "**<TODO: Add diagram>**\n",
    "\n",
    "Here is the model definition for RNN with self attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0365f-7985-46d1-bb5d-79ff9bdefb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNWithAttention(PreTrainedModel):\n",
    "    config_class = AttentionConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.rnn = nn.RNN(config.hidden_size, config.hidden_size, batch_first=True)\n",
    "        self.attention = Attention(config.hidden_size, config.hidden_size, config.hidden_size)\n",
    "        self.linear = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "\n",
    "        batch_size, seq_length = input_ids.shape\n",
    "        embedded = self.embedding(input_ids)\n",
    "\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        outputs= []\n",
    "\n",
    "        attention=None\n",
    "        for t in range(seq_length):\n",
    "            # get current hidden state\n",
    "            hidden = rnn_output[:, t,:].unsqueeze(1) # [Bx1xH]\n",
    "\n",
    "            # apply attention\n",
    "            attention, weighted_value = self.attention(\n",
    "                    hidden,\n",
    "                    rnn_output[:,:t+1,:],\n",
    "                    rnn_output[:,:t+1,:]\n",
    "            )\n",
    "\n",
    "            # generate output\n",
    "            output = self.linear(weighted_value)\n",
    "            outputs.append(output)\n",
    "\n",
    "        logits = torch.stack(outputs,dim=1)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = logits[...,:-1,:].contiguous()\n",
    "            shift_labels = labels[...,1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            attentions=attention\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        return {\n",
    "            \"input_ids\": input_ids\n",
    "        }\n",
    "\n",
    "    def manual_generate(\n",
    "            self, \n",
    "            input_text, \n",
    "            tokenizer, \n",
    "            max_length=50,\n",
    "            temperature=1.0,\n",
    "        ):\n",
    "            \"\"\"\n",
    "            Generates text based on the provided input.\n",
    "    \n",
    "            Args:\n",
    "                input_text (str): The initial text to start generation from.\n",
    "                tokenizer: The HuggingFace tokenizer for the model's vocabulary.\n",
    "                max_length (int, optional): The maximum length of generated text. Defaults to 50.\n",
    "                temperature (float, optional): Controls the randomness of the generated text. \n",
    "                    Higher values make the output more random. Defaults to 1.0.\n",
    "    \n",
    "            Returns:\n",
    "                str: The generated text sequence.\n",
    "            \"\"\"\n",
    "    \n",
    "            input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(self.device)\n",
    "    \n",
    "            generated_sequence = input_ids[0]\n",
    "            for _ in range(max_length - len(input_ids[0])):\n",
    "                with torch.no_grad():\n",
    "                    output = self.forward(generated_sequence.unsqueeze(0))\n",
    "                    logits = output.logits[0, -1, :] / temperature \n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    generated_sequence = torch.cat((generated_sequence, next_token), dim=0)\n",
    "    \n",
    "            return tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past, beam_idx):\n",
    "        return past\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b747463-7da1-46fe-b871-3310bebadaa3",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e6157-0045-435e-86f1-8602e8914854",
   "metadata": {},
   "source": [
    "To train our model on the intricacies of language, we'll leverage the powerful Hugging Face Trainer API. We'll use a publicly available dataset containing wikipedia articles. This is usually a dump of all the articles made on a specific date. Our goal is to learn the language structure with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd48f90d-6f6a-42fc-b66e-266de8bb54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f6b840-4963-4e4d-91d9-24f7dad8cc2b",
   "metadata": {},
   "source": [
    "To tokenize, we will use `Huggingface Tokenizers`. This knows how to parse the raw text and convert it into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f065b4-e2f8-4eb6-ad56-b541b54c6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
    "\n",
    "\n",
    "tokenized_datasets = (dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac012ea-2b16-4c98-a97d-ac4174ed9851",
   "metadata": {},
   "source": [
    "This is huggingface specific. We need to create config for each model to train. This config contains model parameters to be used for initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "685d0624-6f1f-46eb-b9d9-b1b9a0258bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and configure training\n",
    "config = AttentionConfig()\n",
    "model = RNNWithAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "40d14a1c-3aa7-49a4-93ea-b898bc4a8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    #gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    learning_rate=1e-2,\n",
    "    optim=\"adafactor\"\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73391362-738d-49e3-855b-218d97e32d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fda9f350-5427-4595-aa05-0fdc1e368691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2870' max='2870' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2870/2870 07:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.645400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>4.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>4.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.992700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.997100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.833600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.653100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2870, training_loss=4.304410098404834, metrics={'train_runtime': 462.0655, 'train_samples_per_second': 794.649, 'train_steps_per_second': 6.211, 'total_flos': 1780264886400000.0, 'train_loss': 4.304410098404834, 'epoch': 10.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "20c0d74e-ed3f-4901-b334-90409bcdf8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('bin/model_128_006')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77989482-2e41-4803-a03b-52c86873c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNWithAttention.from_pretrained('bin/model_128_006')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef2b51-266c-4328-9f01-3fc7566c1810",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77ab584e-2cb8-4a0a-b55f-90d2bed0937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c8bda71-bd63-452f-b9fb-f5d0e65c283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox, the long @-@ brick terrace, Ders conceded varsity coach. He did this lives, as well asigned and roll, microorganisms, specifically females leave the term F @-@ for death in a Comedy Series, in the United States's appointment at Artistsers, John photography Blues Lane â€“ a daughter and Co @-@ nation tropes often referred to as the mainchandised Princess aviation, and had to purchase a heavier solution will to rarity with\n"
     ]
    }
   ],
   "source": [
    "generated_texts = model.manual_generate(\n",
    "    \"The quick brown fox\", tokenizer, max_length=100\n",
    ")\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a24ab8c7-0711-4ff9-b795-19073bb6befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'RNNWithAttention' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "generate_text = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "166fd404-c188-4f42-88ce-4fc74356b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "result = generate_text(\"The quick brown fox\", max_length =50, do_sample=True, top_k =50, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3c08da9-3994-4616-a561-1e5b58d051ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown foxes, although the wild of the prerogative is usually harmful to the consumption of \" Like the song \" the \" best @-@ pop song \", and \" Ode to Psyche \", which \", she has\n"
     ]
    }
   ],
   "source": [
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dccef63f-448a-4957-8f64-b5ae89460d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_f = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b517676c-eb9a-4836-bc1a-e59c5e72b791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.330234527587891,\n",
       " 'eval_runtime': 1.4481,\n",
       " 'eval_samples_per_second': 2596.464,\n",
       " 'eval_steps_per_second': 20.716,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e385f1dc-38e5-4ce1-95b7-d4b90a7921f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "590e8d77-008f-4163-bbd6-1985521aff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_f = math.exp(eval_f['eval_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0d9dcb43-b26f-4710-95c8-2788a8c7ff95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561.2882159892837"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fefe49-769e-4f00-8713-b23124408e04",
   "metadata": {},
   "source": [
    "## Transformer: Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c229d6-1567-45d2-b6f2-fd8d389507a3",
   "metadata": {},
   "source": [
    "In earlier section, we saw simple RNN model struggling to learn the language but attention model gave a high boost to the language model. There are few drawbacks of RNN with self attention as mentioned below:\n",
    "\n",
    "- **Computation time**\n",
    "- **Only one self attention head**\n",
    "- **Vanishing gradient**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b45a06-4251-467a-acb3-bdd591918a8e",
   "metadata": {},
   "source": [
    "Transformer architecture is built on RNN structures but without above flaws. It does this very cleverly by following a unique mechanism to find the recurrence relation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bc0b2-5af7-4170-88b6-63508fe83566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
